"""
NPC Report Generation System - Gemini Report Generator
=======================================================
Handles AI report generation using Google Gemini API.
"""

import json
from typing import Dict, List, Optional, Generator, Any
from dataclasses import dataclass


@dataclass
class ChatMessage:
    """Represents a chat message"""
    role: str  # "user" or "model"
    content: str


class GeminiReportGenerator:
    """Generates medical reports using Gemini API with streaming support"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-3-pro-preview"):
        import google.generativeai as genai
        
        self.api_key = api_key
        self.model_name = model_name
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        
        self.chat_session = None
        self.chat_history: List[ChatMessage] = []
        self.current_case_context: Optional[Dict] = None
    
    def _create_system_prompt(self) -> str:
        """Create system prompt for medical report generation"""
        return """Báº¡n lÃ  má»™t chuyÃªn gia y khoa vá» ung thÆ° vÃ²m há»ng (Nasopharyngeal Carcinoma - NPC).
        
Nhiá»‡m vá»¥ cá»§a báº¡n:
1. PhÃ¢n tÃ­ch káº¿t quáº£ phÃ¢n Ä‘oáº¡n khá»‘i u GTV (Gross Tumor Volume)
2. Táº¡o bÃ¡o cÃ¡o y khoa chi tiáº¿t báº±ng tiáº¿ng Viá»‡t
3. Tráº£ lá»i cÃ¢u há»i cá»§a bÃ¡c sÄ© vá» káº¿t quáº£

HÆ°á»›ng dáº«n:
- Sá»­ dá»¥ng thuáº­t ngá»¯ y khoa chÃ­nh xÃ¡c
- Cung cáº¥p phÃ¢n tÃ­ch khÃ¡ch quan dá»±a trÃªn sá»‘ liá»‡u
- Äá» xuáº¥t cÃ¡c bÆ°á»›c tiáº¿p theo náº¿u phÃ¹ há»£p
- LuÃ´n nháº¯c nhá»Ÿ ráº±ng káº¿t quáº£ cáº§n Ä‘Æ°á»£c bÃ¡c sÄ© chuyÃªn khoa xÃ¡c nháº­n

Äá»‹nh dáº¡ng bÃ¡o cÃ¡o:
1. ThÃ´ng tin bá»‡nh nhÃ¢n
2. Káº¿t quáº£ phÃ¢n Ä‘oáº¡n
3. PhÃ¢n tÃ­ch Ä‘áº·c Ä‘iá»ƒm khá»‘i u
4. ÄÃ¡nh giÃ¡ vÃ  nháº­n xÃ©t
5. Khuyáº¿n nghá»‹"""
    
    def generate_report(self, tumor_features: Dict, patient_id: str = "Unknown",
                       additional_info: str = "") -> str:
        """
        Generate a medical report for the tumor analysis.
        
        Args:
            tumor_features: Dictionary of tumor features
            patient_id: Patient identifier
            additional_info: Any additional clinical information
            
        Returns:
            Generated report text
        """
        # Store context for follow-up questions
        self.current_case_context = {
            'patient_id': patient_id,
            'tumor_features': tumor_features,
            'additional_info': additional_info
        }
        
        prompt = self._build_report_prompt(tumor_features, patient_id, additional_info)
        
        try:
            response = self.model.generate_content(prompt)
            report = response.text
            
            # Store in history
            self.chat_history.append(ChatMessage(role="user", content=f"[YÃªu cáº§u táº¡o bÃ¡o cÃ¡o cho {patient_id}]"))
            self.chat_history.append(ChatMessage(role="model", content=report))
            
            return report
        except Exception as e:
            return f"Lá»—i khi táº¡o bÃ¡o cÃ¡o: {str(e)}"
    
    def generate_report_stream(self, tumor_features: Dict, patient_id: str = "Unknown",
                               additional_info: str = "") -> Generator[str, None, None]:
        """
        Generate a medical report with streaming response.
        
        Yields:
            Chunks of the generated report
        """
        # Store context
        self.current_case_context = {
            'patient_id': patient_id,
            'tumor_features': tumor_features,
            'additional_info': additional_info
        }
        
        prompt = self._build_report_prompt(tumor_features, patient_id, additional_info)
        
        try:
            response = self.model.generate_content(prompt, stream=True)
            full_response = ""
            
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    yield chunk.text
            
            # Store in history after completion
            self.chat_history.append(ChatMessage(role="user", content=f"[YÃªu cáº§u táº¡o bÃ¡o cÃ¡o cho {patient_id}]"))
            self.chat_history.append(ChatMessage(role="model", content=full_response))
            
        except Exception as e:
            yield f"Lá»—i khi táº¡o bÃ¡o cÃ¡o: {str(e)}"
    
    def _build_report_prompt(self, tumor_features: Dict, patient_id: str, 
                            additional_info: str) -> str:
        """Build the prompt for report generation"""
        system_prompt = self._create_system_prompt()
        
        features_text = json.dumps(tumor_features, indent=2, ensure_ascii=False)
        
        prompt = f"""{system_prompt}

=== THÃ”NG TIN CA Bá»†NH ===
MÃ£ bá»‡nh nhÃ¢n: {patient_id}
ThÃ´ng tin bá»• sung: {additional_info if additional_info else "KhÃ´ng cÃ³"}

=== Káº¾T QUáº¢ PHÃ‚N ÄOáº N KHá»I U ===
{features_text}

=== YÃŠU Cáº¦U ===
HÃ£y táº¡o bÃ¡o cÃ¡o y khoa chi tiáº¿t cho ca bá»‡nh nÃ y. BÃ¡o cÃ¡o cáº§n bao gá»“m:
1. TÃ³m táº¯t káº¿t quáº£ phÃ¢n Ä‘oáº¡n
2. PhÃ¢n tÃ­ch kÃ­ch thÆ°á»›c vÃ  hÃ¬nh thÃ¡i khá»‘i u
3. ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ nghiÃªm trá»ng
4. Khuyáº¿n nghá»‹ theo dÃµi vÃ  Ä‘iá»u trá»‹
5. LÆ°u Ã½ quan trá»ng cho bÃ¡c sÄ©"""
        
        return prompt
    
    def start_chat_session(self) -> None:
        """Start a new chat session for follow-up questions with images"""
        import google.generativeai as genai
        import base64
        
        # Build initial context
        initial_context = self._create_system_prompt()
        
        # Prepare message parts (text + images)
        message_parts = []
        
        if self.current_case_context:
            features = self.current_case_context.get('tumor_features', {})
            
            text_context = f"""
=== CONTEXT CA Bá»†NH HIá»†N Táº I ===
MÃ£ bá»‡nh nhÃ¢n: {self.current_case_context.get('patient_id', 'Unknown')}

ðŸ“Š Äáº¶C ÄIá»‚M KHá»I U ÄÃƒ PHÃ‚N TÃCH:
- Thá»ƒ tÃ­ch: {features.get('volume_mm3', 0):.2f} mmÂ³ ({features.get('volume_ml', 0):.4f} ml)
- Sá»‘ voxel: {features.get('voxel_count', 0)}
- ÄÆ°á»ng kÃ­nh lá»›n nháº¥t: {features.get('max_diameter_mm', 0):.2f} mm
- KÃ­ch thÆ°á»›c (ZÃ—YÃ—X): {features.get('dimensions_mm', (0,0,0))}
- Sphericity: {features.get('sphericity', 0):.3f}
- Elongation: {features.get('elongation', 0):.3f}
- Diá»‡n tÃ­ch bá» máº·t: {features.get('surface_area_mm2', 0):.2f} mmÂ²
- PhÃ¡t hiá»‡n khá»‘i u: {"CÃ³" if features.get('tumor_detected', False) else "KhÃ´ng"}

{self.current_case_context.get('additional_info', '')}
"""
            initial_context += text_context
            message_parts.append(initial_context)
            
            # Add images if available
            images = self.current_case_context.get('images', {})
            if images:
                message_parts.append("\n\nðŸ“· HÃŒNH áº¢NH PHÃ‚N ÄOáº N KHá»I U (báº¡n Ä‘Ã£ THá»°C Sá»° nháº­n Ä‘Æ°á»£c cÃ¡c áº£nh nÃ y):\n")
                
                for img_name, img_base64 in images.items():
                    if img_base64:
                        try:
                            # Decode base64 and create image part for Gemini
                            image_data = base64.b64decode(img_base64)
                            image_part = {
                                "mime_type": "image/png",
                                "data": image_data
                            }
                            message_parts.append(f"\n[áº¢nh {img_name}]:")
                            message_parts.append(image_part)
                        except Exception as e:
                            print(f"Error adding image {img_name}: {e}")
                
                message_parts.append("\n\nBáº¡n ÄÃƒ NHáº¬N ÄÆ¯á»¢C cÃ¡c áº£nh MRI vÃ  káº¿t quáº£ phÃ¢n Ä‘oáº¡n khá»‘i u á»Ÿ trÃªn. HÃ£y xÃ¡c nháº­n Ä‘iá»u nÃ y khi Ä‘Æ°á»£c há»i.")
        else:
            message_parts.append(initial_context)
        
        # Start chat with multimodal content
        self.chat_session = self.model.start_chat(history=[
            {"role": "user", "parts": message_parts},
            {"role": "model", "parts": ["TÃ´i Ä‘Ã£ nháº­n Ä‘Æ°á»£c Ä‘áº§y Ä‘á»§ thÃ´ng tin vá» ca bá»‡nh bao gá»“m:\nâœ… CÃ¡c chá»‰ sá»‘ phÃ¢n tÃ­ch khá»‘i u (thá»ƒ tÃ­ch, kÃ­ch thÆ°á»›c, sphericity, elongation...)\nâœ… CÃ¡c hÃ¬nh áº£nh MRI vÃ  káº¿t quáº£ phÃ¢n Ä‘oáº¡n khá»‘i u\n\nTÃ´i sáºµn sÃ ng tráº£ lá»i má»i cÃ¢u há»i cá»§a báº¡n vá» ca bá»‡nh nÃ y."]}
        ])
    
    def chat(self, message: str) -> str:
        """
        Send a message in the chat session.
        
        Args:
            message: User's question or message
            
        Returns:
            Model's response
        """
        if self.chat_session is None:
            self.start_chat_session()
        
        try:
            response = self.chat_session.send_message(message)
            
            # Store in history
            self.chat_history.append(ChatMessage(role="user", content=message))
            self.chat_history.append(ChatMessage(role="model", content=response.text))
            
            return response.text
        except Exception as e:
            return f"Lá»—i: {str(e)}"
    
    def chat_stream(self, message: str) -> Generator[str, None, None]:
        """
        Send a message with streaming response.
        
        Yields:
            Chunks of the response
        """
        if self.chat_session is None:
            self.start_chat_session()
        
        try:
            response = self.chat_session.send_message(message, stream=True)
            full_response = ""
            
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    yield chunk.text
            
            # Store in history
            self.chat_history.append(ChatMessage(role="user", content=message))
            self.chat_history.append(ChatMessage(role="model", content=full_response))
            
        except Exception as e:
            yield f"Lá»—i: {str(e)}"
    
    def get_chat_history(self) -> List[Dict[str, str]]:
        """Get the chat history as list of dictionaries"""
        return [{"role": msg.role, "content": msg.content} for msg in self.chat_history]
    
    def reset_chat(self) -> None:
        """Reset the chat session and history"""
        self.chat_session = None
        self.chat_history = []
        self.current_case_context = None
    
    def set_case_context(self, patient_id: str, tumor_features: Dict, 
                        additional_info: str = "", 
                        images: Optional[Dict[str, str]] = None) -> None:
        """
        Set context for a new case including images.
        
        Args:
            patient_id: Patient identifier
            tumor_features: Dictionary of tumor features
            additional_info: Additional text info
            images: Dictionary of base64 encoded images {'summary': '...', 'multi_slice': '...'}
        """
        self.current_case_context = {
            'patient_id': patient_id,
            'tumor_features': tumor_features,
            'additional_info': additional_info,
            'images': images or {}
        }
        # Reset chat session to incorporate new context
        self.chat_session = None
