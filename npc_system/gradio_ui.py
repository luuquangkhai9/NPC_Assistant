"""
NPC Report Generation System - Gradio Web UI
=============================================
Interactive web interface for the NPC report generation system.
"""

import os
import json
import base64
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any
import tempfile

import gradio as gr

from .config import get_config, update_gemini_api_key
from .pipeline import NPCReportPipeline


# ============================================================
# Global State
# ============================================================

pipeline: Optional[NPCReportPipeline] = None
chat_history: List[Dict[str, str]] = []  # Gradio 6.0 uses messages format


def get_pipeline() -> NPCReportPipeline:
    """Get or create the pipeline instance"""
    global pipeline
    if pipeline is None:
        pipeline = NPCReportPipeline()
    return pipeline


def initialize_system(api_key: str) -> str:
    """Initialize the system with API key"""
    global pipeline
    
    if not api_key.strip():
        return "‚ùå Vui l√≤ng nh·∫≠p Gemini API Key"
    
    try:
        update_gemini_api_key(api_key.strip())
        pipeline = NPCReportPipeline()
        success = pipeline.initialize()
        
        if success:
            return "‚úÖ H·ªá th·ªëng ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng!"
        else:
            return "‚ùå L·ªói kh·ªüi t·∫°o. Ki·ªÉm tra model path."
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"


def get_available_cases() -> gr.Dropdown:
    """Get list of available cases and return updated dropdown"""
    p = get_pipeline()
    if not p.is_initialized:
        return gr.Dropdown(choices=[], value=None)
    
    cases = p.list_available_cases()
    all_cases = []
    
    for name in cases.get('test', []):
        all_cases.append(f"[TEST] {name}")
    for name in cases.get('val', []):
        all_cases.append(f"[VAL] {name}")
    
    return gr.Dropdown(choices=all_cases, value=None)


def process_selected_case(case_selection: str, progress=gr.Progress()) -> Tuple:
    """Process a selected case from the dropdown"""
    global chat_history
    chat_history = []
    
    if not case_selection:
        return None, None, None, "", "‚ùå Ch·ªçn m·ªôt case ƒë·ªÉ x·ª≠ l√Ω", []
    
    p = get_pipeline()
    if not p.is_initialized:
        return None, None, None, "", "‚ùå H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o", []
    
    # Parse selection
    if case_selection.startswith("[TEST]"):
        dataset = "test"
        filename = case_selection.replace("[TEST] ", "")
    else:
        dataset = "val"
        filename = case_selection.replace("[VAL] ", "")
    
    case_path = p.get_case_path(filename, dataset)
    if not case_path:
        return None, None, None, "", f"‚ùå Kh√¥ng t√¨m th·∫•y file: {filename}", []
    
    # Process with progress updates
    results = {}
    features_text = ""
    report_text = ""
    patient_id = ""
    
    try:
        progress(0.1, desc="ƒêang t·∫£i d·ªØ li·ªáu...")
        
        for update in p.process_case_stream(case_path):
            step = update.get('step', '')
            msg = update.get('message', '')
            prog = update.get('progress', 0) / 100
            
            progress(prog, desc=msg)
            
            if step == 'loaded':
                patient_id = update.get('message', '').replace('ƒê√£ t·∫£i: ', '')
            
            if step == 'analyzed':
                results['features'] = update.get('features', {})
                features_text = format_features(results['features'])
            
            if step == 'visualized':
                results['visualizations'] = update.get('visualizations', {})
            
            if step == 'report_chunk':
                report_text += update.get('chunk', '')
            
            if step == 'reported':
                report_text = update.get('report', report_text)
            
            if step == 'completed':
                patient_id = update.get('patient_id', patient_id)
            
            if step == 'error':
                return None, None, None, "", f"‚ùå {update.get('error', 'Unknown error')}", []
        
        # Decode images
        img_multi = decode_base64_image(results.get('visualizations', {}).get('multi_slice', ''))
        img_3plane = decode_base64_image(results.get('visualizations', {}).get('three_plane', ''))
        img_summary = decode_base64_image(results.get('visualizations', {}).get('summary', ''))
        
        # === T·∫†O CHAT HISTORY V·ªöI B√ÅO C√ÅO ===
        # Kh·ªüi t·∫°o chat v·ªõi context ƒë·∫ßy ƒë·ªß v·ªÅ case v·ª´a x·ª≠ l√Ω
        chat_history = [
            {
                "role": "assistant", 
                "content": f"""üè• **ƒê√É HO√ÄN TH√ÄNH PH√ÇN T√çCH CA B·ªÜNH: {patient_id}**

T√¥i ƒë√£ nh·∫≠n ƒë∆∞·ª£c ƒë·∫ßy ƒë·ªß th√¥ng tin v·ªÅ ca b·ªánh n√†y bao g·ªìm:
- ‚úÖ ·∫¢nh MRI v√† k·∫øt qu·∫£ ph√¢n ƒëo·∫°n kh·ªëi u
- ‚úÖ C√°c ch·ªâ s·ªë ƒë·∫∑c ƒëi·ªÉm kh·ªëi u (th·ªÉ t√≠ch, k√≠ch th∆∞·ªõc, h√¨nh th√°i)
- ‚úÖ H√¨nh ·∫£nh tr·ª±c quan h√≥a (multi-slice, 3-plane view)

---

{report_text}

---

üí¨ **B·∫°n c√≥ th·ªÉ h·ªèi t√¥i b·∫•t k·ª≥ c√¢u h·ªèi n√†o v·ªÅ:**
- √ù nghƒ©a c√°c ch·ªâ s·ªë (sphericity, elongation, th·ªÉ t√≠ch...)
- ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng c·ªßa kh·ªëi u
- Khuy·∫øn ngh·ªã theo d√µi v√† ƒëi·ªÅu tr·ªã
- So s√°nh v·ªõi c√°c ca t∆∞∆°ng t·ª±
- Gi·∫£i th√≠ch chi ti·∫øt v·ªÅ b√°o c√°o"""
            }
        ]
        
        # ƒê·∫£m b·∫£o Gemini service c√≥ context ƒë·∫ßy ƒë·ªß V·ªöI ·∫¢NH
        if p._gemini:
            # L·∫•y ·∫£nh base64 ƒë·ªÉ g·ª≠i v√†o Gemini
            images_for_gemini = {
                'summary': results.get('visualizations', {}).get('summary', ''),
                'multi_slice': results.get('visualizations', {}).get('multi_slice', ''),
                'three_plane': results.get('visualizations', {}).get('three_plane', '')
            }
            p._gemini.set_case_context(
                patient_id=patient_id,
                tumor_features=results.get('features', {}),
                additional_info=f"B√°o c√°o ƒë√£ t·∫°o: {report_text[:500]}...",
                images=images_for_gemini  # G·ª≠i ·∫£nh th·ª±c t·∫ø
            )
        
        return img_multi, img_3plane, img_summary, features_text, report_text, chat_history
        
    except Exception as e:
        return None, None, None, "", f"‚ùå L·ªói x·ª≠ l√Ω: {str(e)}", []


def process_uploaded_file(file, progress=gr.Progress()) -> Tuple:
    """Process an uploaded HDF5 file"""
    global chat_history
    chat_history = []
    
    if file is None:
        return None, None, None, "", "‚ùå Ch·ªçn file ƒë·ªÉ upload", []
    
    p = get_pipeline()
    if not p.is_initialized:
        return None, None, None, "", "‚ùå H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o", []
    
    try:
        # Process uploaded file
        file_path = Path(file.name)
        
        results = {}
        features_text = ""
        report_text = ""
        patient_id = ""
        
        progress(0.1, desc="ƒêang t·∫£i d·ªØ li·ªáu...")
        
        for update in p.process_case_stream(file_path):
            step = update.get('step', '')
            msg = update.get('message', '')
            prog = update.get('progress', 0) / 100
            
            progress(prog, desc=msg)
            
            if step == 'loaded':
                patient_id = update.get('message', '').replace('ƒê√£ t·∫£i: ', '')
            
            if step == 'analyzed':
                results['features'] = update.get('features', {})
                features_text = format_features(results['features'])
            
            if step == 'visualized':
                results['visualizations'] = update.get('visualizations', {})
            
            if step == 'report_chunk':
                report_text += update.get('chunk', '')
            
            if step == 'reported':
                report_text = update.get('report', report_text)
            
            if step == 'completed':
                patient_id = update.get('patient_id', patient_id)
            
            if step == 'error':
                return None, None, None, "", f"‚ùå {update.get('error', 'Unknown error')}", []
        
        # Decode images
        img_multi = decode_base64_image(results.get('visualizations', {}).get('multi_slice', ''))
        img_3plane = decode_base64_image(results.get('visualizations', {}).get('three_plane', ''))
        img_summary = decode_base64_image(results.get('visualizations', {}).get('summary', ''))
        
        # === T·∫†O CHAT HISTORY V·ªöI B√ÅO C√ÅO ===
        chat_history = [
            {
                "role": "assistant", 
                "content": f"""üè• **ƒê√É HO√ÄN TH√ÄNH PH√ÇN T√çCH CA B·ªÜNH: {patient_id}**

T√¥i ƒë√£ nh·∫≠n ƒë∆∞·ª£c ƒë·∫ßy ƒë·ªß th√¥ng tin v·ªÅ ca b·ªánh n√†y bao g·ªìm:
- ‚úÖ ·∫¢nh MRI v√† k·∫øt qu·∫£ ph√¢n ƒëo·∫°n kh·ªëi u
- ‚úÖ C√°c ch·ªâ s·ªë ƒë·∫∑c ƒëi·ªÉm kh·ªëi u (th·ªÉ t√≠ch, k√≠ch th∆∞·ªõc, h√¨nh th√°i)
- ‚úÖ H√¨nh ·∫£nh tr·ª±c quan h√≥a (multi-slice, 3-plane view)

---

{report_text}

---

üí¨ **B·∫°n c√≥ th·ªÉ h·ªèi t√¥i b·∫•t k·ª≥ c√¢u h·ªèi n√†o v·ªÅ:**
- √ù nghƒ©a c√°c ch·ªâ s·ªë (sphericity, elongation, th·ªÉ t√≠ch...)
- ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng c·ªßa kh·ªëi u
- Khuy·∫øn ngh·ªã theo d√µi v√† ƒëi·ªÅu tr·ªã
- So s√°nh v·ªõi c√°c ca t∆∞∆°ng t·ª±
- Gi·∫£i th√≠ch chi ti·∫øt v·ªÅ b√°o c√°o"""
            }
        ]
        
        # ƒê·∫£m b·∫£o Gemini service c√≥ context ƒë·∫ßy ƒë·ªß V·ªöI ·∫¢NH
        if p._gemini:
            # L·∫•y ·∫£nh base64 ƒë·ªÉ g·ª≠i v√†o Gemini
            images_for_gemini = {
                'summary': results.get('visualizations', {}).get('summary', ''),
                'multi_slice': results.get('visualizations', {}).get('multi_slice', ''),
                'three_plane': results.get('visualizations', {}).get('three_plane', '')
            }
            p._gemini.set_case_context(
                patient_id=patient_id,
                tumor_features=results.get('features', {}),
                additional_info=f"B√°o c√°o ƒë√£ t·∫°o: {report_text[:500]}...",
                images=images_for_gemini  # G·ª≠i ·∫£nh th·ª±c t·∫ø
            )
        
        return img_multi, img_3plane, img_summary, features_text, report_text, chat_history
        
    except Exception as e:
        return None, None, None, "", f"‚ùå L·ªói: {str(e)}", []


def chat_with_ai(message: str, history: List[Dict[str, str]]) -> Tuple[str, List[Dict[str, str]]]:
    """Handle chat interaction - Gradio 6.0 messages format"""
    global chat_history
    
    if not message.strip():
        return "", history
    
    p = get_pipeline()
    if not p.is_initialized:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "‚ùå H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o"})
        return "", history
    
    if not p.current_case:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "‚ùå Ch∆∞a c√≥ case n√†o ƒë∆∞·ª£c x·ª≠ l√Ω. Vui l√≤ng x·ª≠ l√Ω m·ªôt case tr∆∞·ªõc."})
        return "", history
    
    try:
        # Get streaming response
        response = ""
        for chunk in p.chat(message, stream=True):
            response += chunk
        
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response})
        chat_history = history
        return "", history
        
    except Exception as e:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": f"‚ùå L·ªói: {str(e)}"})
        return "", history


def reset_chat_history() -> List[Dict[str, str]]:
    """Reset chat history"""
    global chat_history
    chat_history = []
    
    p = get_pipeline()
    if p and p.is_initialized:
        p.reset_chat()
    
    return []


def format_features(features: dict) -> str:
    """Format features for display"""
    if not features:
        return "Kh√¥ng c√≥ d·ªØ li·ªáu"
    
    text = """
üìä **ƒê·∫∂C ƒêI·ªÇM KH·ªêI U**

üî¨ **Th·ªÉ t√≠ch:**
- Volume: {volume_mm3:.2f} mm¬≥ ({volume_ml:.4f} ml)
- S·ªë voxel: {voxel_count}

üìè **K√≠ch th∆∞·ªõc:**
- ƒê∆∞·ªùng k√≠nh l·ªõn nh·∫•t: {max_diameter_mm:.2f} mm
- K√≠ch th∆∞·ªõc (Z√óY√óX): {dim_z:.1f} √ó {dim_y:.1f} √ó {dim_x:.1f} mm

üîµ **H√¨nh th√°i:**
- Sphericity: {sphericity:.3f}
- Elongation: {elongation:.3f}
- Di·ªán t√≠ch b·ªÅ m·∫∑t: {surface_area_mm2:.2f} mm¬≤

‚úÖ **Ph√°t hi·ªán kh·ªëi u:** {tumor_detected}
""".format(
        volume_mm3=features.get('volume_mm3', 0),
        volume_ml=features.get('volume_ml', 0),
        voxel_count=features.get('voxel_count', 0),
        max_diameter_mm=features.get('max_diameter_mm', 0),
        dim_z=features.get('dimensions_mm', (0,0,0))[0],
        dim_y=features.get('dimensions_mm', (0,0,0))[1],
        dim_x=features.get('dimensions_mm', (0,0,0))[2],
        sphericity=features.get('sphericity', 0),
        elongation=features.get('elongation', 0),
        surface_area_mm2=features.get('surface_area_mm2', 0),
        tumor_detected="C√≥" if features.get('tumor_detected', False) else "Kh√¥ng"
    )
    return text


def decode_base64_image(base64_str: str):
    """Decode base64 string to image"""
    if not base64_str:
        return None
    
    try:
        import io
        from PIL import Image
        
        image_data = base64.b64decode(base64_str)
        image = Image.open(io.BytesIO(image_data))
        return image
    except Exception:
        return None


def create_gradio_app() -> gr.Blocks:
    """Create the Gradio interface"""
    
    with gr.Blocks(title="NPC Tumor Report Generation") as app:
        
        # Header
        gr.HTML("""
        <div class="main-header">
            <h1>üè• NPC Tumor Report Generation System</h1>
            <p>H·ªá th·ªëng ph√¢n t√≠ch v√† t·∫°o b√°o c√°o kh·ªëi u v√≤m h·ªçng s·ª≠ d·ª•ng AI</p>
        </div>
        """)
        
        # Initialization section
        with gr.Row():
            with gr.Column(scale=3):
                api_key_input = gr.Textbox(
                    label="Gemini API Key",
                    placeholder="Nh·∫≠p API key c·ªßa b·∫°n...",
                    type="password"
                )
            with gr.Column(scale=1):
                init_btn = gr.Button("üöÄ Kh·ªüi t·∫°o h·ªá th·ªëng", variant="primary")
            with gr.Column(scale=2):
                init_status = gr.Textbox(label="Tr·∫°ng th√°i", interactive=False)
        
        gr.Markdown("---")
        
        # Main tabs
        with gr.Tabs():
            
            # Tab 1: Process Cases
            with gr.Tab("üìÅ X·ª≠ l√Ω Case"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### Ch·ªçn case c√≥ s·∫µn")
                        case_dropdown = gr.Dropdown(
                            label="Case",
                            choices=[],
                            interactive=True,
                            allow_custom_value=True  # Fix warning
                        )
                        refresh_btn = gr.Button("üîÑ L√†m m·ªõi danh s√°ch")
                        process_btn = gr.Button("‚ñ∂Ô∏è X·ª≠ l√Ω case", variant="primary")
                        
                        gr.Markdown("### Ho·∫∑c upload file")
                        file_upload = gr.File(
                            label="Upload HDF5 file",
                            file_types=[".h5"]
                        )
                        upload_btn = gr.Button("üì§ Upload v√† x·ª≠ l√Ω")
                    
                    with gr.Column(scale=2):
                        gr.Markdown("### K·∫øt qu·∫£ ph√¢n t√≠ch")
                        features_display = gr.Markdown(label="ƒê·∫∑c ƒëi·ªÉm kh·ªëi u")
                
                gr.Markdown("### üìä H√¨nh ·∫£nh tr·ª±c quan")
                with gr.Row():
                    img_multi = gr.Image(label="Multi-Slice View", type="pil")
                    img_3plane = gr.Image(label="3-Plane View", type="pil")
                
                with gr.Row():
                    img_summary = gr.Image(label="Summary Figure", type="pil")
                
                gr.Markdown("### üìù B√°o c√°o AI")
                report_display = gr.Markdown(label="B√°o c√°o")
            
            # Tab 2: Chat
            with gr.Tab("üí¨ H·ªèi ƒë√°p AI"):
                gr.Markdown("""
                ### Chat v·ªõi AI v·ªÅ k·∫øt qu·∫£ ph√¢n t√≠ch
                
                B·∫°n c√≥ th·ªÉ h·ªèi c√°c c√¢u h·ªèi nh∆∞:
                - *"Gi·∫£i th√≠ch √Ω nghƒ©a c·ªßa sphericity"*
                - *"Kh·ªëi u n√†y c√≥ nghi√™m tr·ªçng kh√¥ng?"*
                - *"C·∫ßn theo d√µi nh∆∞ th·∫ø n√†o?"*
                """)
                
                chatbot = gr.Chatbot(
                    label="Chat",
                    height=400
                )
                
                with gr.Row():
                    chat_input = gr.Textbox(
                        label="Nh·∫≠p c√¢u h·ªèi",
                        placeholder="H·ªèi v·ªÅ k·∫øt qu·∫£ ph√¢n t√≠ch...",
                        scale=4 
                    )
                    chat_btn = gr.Button("G·ª≠i", variant="primary", scale=1)
                
                clear_chat_btn = gr.Button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat")
            
            # Tab 3: Settings
            with gr.Tab("‚öôÔ∏è C√†i ƒë·∫∑t"):
                gr.Markdown("### C·∫•u h√¨nh h·ªá th·ªëng")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("""
                        **Th√¥ng tin h·ªá th·ªëng:**
                        - Model: U-Net cho ph√¢n ƒëo·∫°n GTV
                        - AI: Gemini 2.0 Flash
                        - ƒê·ªãnh d·∫°ng input: HDF5
                        """)
                    
                    with gr.Column():
                        gr.Markdown("""
                        **H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**
                        1. Nh·∫≠p Gemini API Key v√† kh·ªüi t·∫°o
                        2. Ch·ªçn case ho·∫∑c upload file
                        3. Xem k·∫øt qu·∫£ v√† b√°o c√°o
                        4. Chat ƒë·ªÉ h·ªèi th√™m v·ªÅ k·∫øt qu·∫£
                        """)
        
        # Event handlers
        init_btn.click(
            fn=initialize_system,
            inputs=[api_key_input],
            outputs=[init_status]
        ).then(
            fn=get_available_cases,
            outputs=[case_dropdown]
        )
        
        refresh_btn.click(
            fn=get_available_cases,
            outputs=[case_dropdown]
        )
        
        process_btn.click(
            fn=process_selected_case,
            inputs=[case_dropdown],
            outputs=[img_multi, img_3plane, img_summary, features_display, report_display, chatbot]
        )
        
        upload_btn.click(
            fn=process_uploaded_file,
            inputs=[file_upload],
            outputs=[img_multi, img_3plane, img_summary, features_display, report_display, chatbot]
        )
        
        # Chat handlers
        chat_btn.click(
            fn=chat_with_ai,
            inputs=[chat_input, chatbot],
            outputs=[chat_input, chatbot]
        )
        
        chat_input.submit(
            fn=chat_with_ai,
            inputs=[chat_input, chatbot],
            outputs=[chat_input, chatbot]
        )
        
        clear_chat_btn.click(
            fn=reset_chat_history,
            outputs=[chatbot]
        )
    
    return app


def launch_gradio(share: bool = False, server_port: int = 7860):
    """Launch the Gradio app"""
    app = create_gradio_app()
    
    print(f"\n{'='*60}")
    print("üè• NPC Tumor Report Generation System")
    print(f"{'='*60}")
    print(f"üåê Truy c·∫≠p t·∫°i: http://localhost:{server_port}")
    print(f"   ho·∫∑c: http://127.0.0.1:{server_port}")
    print(f"{'='*60}\n")
    
    app.launch(
        server_name="0.0.0.0",
        server_port=server_port,
        share=share,
        show_error=True,
        quiet=False
    )


if __name__ == "__main__":
    launch_gradio()
